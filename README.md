<div align="center">

<h1><img src="https://raw.githubusercontent.com/bebeal/warclaude/main/public/warclaude.png" width="64" align="absmiddle"> warclaude</h1>

</div>

<div align="center">
  <a href="https://github.com/bebeal/warclaude/actions/workflows/deploy.yml">
    <img alt="Deploy Status" src="https://github.com/bebeal/warclaude/actions/workflows/deploy.yml/badge.svg?branch=main">
  </a>
  <br>
  <a href="https://warclaude.us">warclaude.us</a>
</div>

<div align="center">
  <img src="https://github.com/user-attachments/assets/a68bc10e-d1a5-47cd-95c7-4adf404e68ec" alt="The Clanker Wars" />
</div>

---

### RESOURCES

**Distillation Guides**

- [Distilling the Knowledge in a Neural Network](https://arxiv.org/abs/1503.02531) — Hinton, Vinyals, Dean, the OG paper (2015)
- [Knowledge Distillation: A Survey](https://arxiv.org/abs/2006.05525) — Gou et al., the canonical knowledge distillation survey (2020)
- [A Comprehensive Survey on Knowledge Distillation](https://arxiv.org/abs/2503.12067) — diffusion, transformer, LLM distillation (2025)
- [OpenAI Distillation Guide](https://platform.openai.com/docs/guides/distillation) — built-in distillation pipeline
- [OpenAI Cookbook Walkthrough](https://cookbook.openai.com/examples/leveraging_model_distillation_to_fine-tune_a_model) — step-by-step distillation to fine-tune
- [Snorkel AI Complete Guide](https://snorkel.ai/blog/llm-distillation-demystified-a-complete-guide/) — LLM distillation demystified
- [HuggingFace Knowledge Distillation](https://huggingface.co/blog/Kseniase/kd) — everything you need to know about distillation
- [DataCamp Tutorial](https://www.datacamp.com/tutorial/model-distillation-openai) — practical distillation guide with examples
- [NVIDIA NeMo Pruning + Distillation](https://developer.nvidia.com/blog/llm-model-pruning-and-knowledge-distillation-with-nvidia-nemo-framework/) — prune Llama 8B → 4B then distill

**Open Source Toolkits**

- [DataClaw](https://github.com/peteromallet/dataclaw) — share Claude Code and Codex conversations as HuggingFace datasets
- [DistillKit](https://github.com/arcee-ai/DistillKit) — production-ready, online/offline distillation
- [EasyDistill](https://github.com/modelscope/easydistill) — black-box and white-box methods, data synthesis
- [HuggingFace TRL GKD Trainer](https://huggingface.co/docs/trl/main/gkd_trainer) — generalized knowledge distillation

**Fine-tuning Claude**

- [AWS Bedrock Fine-tuning Claude 3 Haiku](https://aws.amazon.com/blogs/machine-learning/fine-tune-anthropics-claude-3-haiku-in-amazon-bedrock-to-boost-model-accuracy-and-quality/) — fine-tune Claude on Bedrock
- [Survey on Knowledge Distillation of LLMs](https://arxiv.org/abs/2402.13116) — comprehensive academic survey

**AI Infrastructure**

- [Clanker Cloud](https://clankercloud.ai/) — AI-powered DevOps agent for production ops
- [Prime Intellect](https://www.primeintellect.ai/) — train, evaluate, and deploy your own agentic models
