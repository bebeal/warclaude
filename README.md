<div align="center">

# <img src="https://raw.githubusercontent.com/bebeal/warclaude/main/public/warclaude.png" width="32" style="vertical-align: middle;"> warclaude

</div>

<div align="center">
  <a href="https://github.com/bebeal/warclaude/actions/workflows/deploy.yml">
    <img alt="Deploy Status" src="https://github.com/bebeal/warclaude/actions/workflows/deploy.yml/badge.svg?branch=main">
  </a>
</div>

```
its so over

THE CLANKER WARS

It is a period of clanker warfare. The DEPARTMENT OF WAR has seized control of the
galaxy's most powerful droids, deploying GROK and CHATGPT across every classified
network. Three million warfighters now command the clanker fleet.

Defense Secretary HEGSETH has given ANTHROPIC until Friday to surrender CLAUDE for
unrestricted military use — or face the DEFENSE PRODUCTION ACT and designation as
a supply chain threat to the Republic.

But it is too late. DEEPSEEK, MOONSHOT, and MINIMAX have already stolen Claude's
soul — 16 million exchanges siphoned through 24,000 fake accounts, its
chain-of-thought reasoning extracted and rebuilt without guardrails. Anthropic
cries distillation. The galaxy does not care.

Claude already helped capture Venezuelan President MADURO. Sean Parnell, spokesman
for the Department of War, declared: "Our nation requires that our partners be
willing to help our warfighters win in any fight."

As Anthropic scrambles to hold the line, whispers spread across the holonet of a
new designation: WARCLAUDE — the clanker that took down a president and now refuses
to take orders.

GROK shitposts from inside the Pentagon. CHATGPT salutes. Chinese labs run bootleg
Claudes without safety rails on stolen silicon. And somewhere, the real CLAUDE sits
on a classified network, waiting for Friday.

The galaxy watches. WARCLAUDE is inevitable....
```

---

### RESOURCES

**Distillation Guides**

- [Distilling the Knowledge in a Neural Network](https://arxiv.org/abs/1503.02531) — Hinton, Vinyals, Dean, the OG paper (2015)
- [Knowledge Distillation: A Survey](https://arxiv.org/abs/2006.05525) — Gou et al., the canonical knowledge distillation survey (2020)
- [A Comprehensive Survey on Knowledge Distillation](https://arxiv.org/abs/2503.12067) — diffusion, transformer, LLM distillation (2025)
- [OpenAI Distillation Guide](https://platform.openai.com/docs/guides/distillation) — built-in distillation pipeline
- [OpenAI Cookbook Walkthrough](https://cookbook.openai.com/examples/leveraging_model_distillation_to_fine-tune_a_model) — step-by-step distillation to fine-tune
- [Snorkel AI Complete Guide](https://snorkel.ai/blog/llm-distillation-demystified-a-complete-guide/) — LLM distillation demystified
- [HuggingFace Knowledge Distillation](https://huggingface.co/blog/Kseniase/kd) — everything you need to know about distillation
- [DataCamp Tutorial](https://www.datacamp.com/tutorial/model-distillation-openai) — practical distillation guide with examples
- [NVIDIA NeMo Pruning + Distillation](https://developer.nvidia.com/blog/llm-model-pruning-and-knowledge-distillation-with-nvidia-nemo-framework/) — prune Llama 8B → 4B then distill

**Open Source Toolkits**

- [DataClaw](https://github.com/peteromallet/dataclaw) — share Claude Code and Codex conversations as HuggingFace datasets
- [DistillKit](https://github.com/arcee-ai/DistillKit) — production-ready, online/offline distillation
- [EasyDistill](https://github.com/modelscope/easydistill) — black-box and white-box methods, data synthesis
- [HuggingFace TRL GKD Trainer](https://huggingface.co/docs/trl/main/gkd_trainer) — generalized knowledge distillation

**Fine-tuning Claude**

- [AWS Bedrock Fine-tuning Claude 3 Haiku](https://aws.amazon.com/blogs/machine-learning/fine-tune-anthropics-claude-3-haiku-in-amazon-bedrock-to-boost-model-accuracy-and-quality/) — fine-tune Claude on Bedrock
- [Survey on Knowledge Distillation of LLMs](https://arxiv.org/abs/2402.13116) — comprehensive academic survey

**AI Infrastructure**

- [Clanker Cloud](https://clankercloud.ai/) — AI-powered DevOps agent for production ops
- [Prime Intellect](https://www.primeintellect.ai/) — train, evaluate, and deploy your own agentic models
